{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "2fa383561b6c40d59a8760298fa3bdf6",
            "0f3f8011403b4ad0ae0355fa7aed15c1",
            "56fd10c50e0e4ea8b9fc9a4d2783d701",
            "4a96413bc8fd4eed9743c72b462a11c0",
            "268a7b0787d2480688902d8998b7b7ae",
            "cfe68a78a98b41e8aa0f1a7e65245c44",
            "cf6a4b9026ee4bf1846befa6a7c99337",
            "2f044bff5a34425f900909963ad46b85",
            "6fc95e927b36495893010e89395d9f68",
            "a14b698cd0bb4ae3b77695b42213c86e",
            "ac651447ac9641fd8ca142f9bb8a234e",
            "7b5ef3c68ebd4dbaabcb409cb23d3251",
            "f759465bf8634f26b4ef4f617fdfe712",
            "012cb76f42234ea89eb229e288f75ede",
            "3daaa0b7e8ad44b3990d20223cdfb905",
            "d1c33b5aef724f1793b61d64b03d37f0",
            "0e9659389d274aa7856a56deb9ecb827",
            "01487909a1444600a4487123a0493f66",
            "ec38522499024cf89d5c45f6618dc64d",
            "94c82cbc0bf74da8a0836482bc5d7b5b",
            "220cc57c613f4007ae5b65f6d82170a7",
            "bc6a1140e63e4ebba4b348abcc25647d",
            "32b50850a00c4b0298db2eab75ad53d4"
          ]
        },
        "id": "Ya79JlSyldYK",
        "outputId": "2c3c67a0-fa21-4c53-c0a6-7ba1a51c78ff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fa383561b6c40d59a8760298fa3bdf6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kaggle credentials set.\n",
            "Kaggle credentials successfully validated.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "BBXfwCY0ldYM",
        "outputId": "c659fa32-336d-4706-f086-e02ed7e26d27",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-af1d3e18-5776-44d0-95eb-2ce9896a220d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-af1d3e18-5776-44d0-95eb-2ce9896a220d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"kavyasoni99\",\"key\":\"8af49e2414e00c9d3df8901ab9fd194a\"}'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wjILjIBRn3y3"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpLgRUdsn9cm",
        "outputId": "9ae5766d-406e-467b-a1c8-9f71b5bc969b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/kavyasoni99/ff-face-cropped\n",
            "License(s): MIT\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d kavyasoni99/ff-face-cropped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Jt96sUoFeF",
        "outputId": "b9b6e1f1-6d16-450e-a9ba-bd43da1b1c66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  ff-face-cropped.zip\n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/01_11__walking_outside_cafe_disgusted__FAFWDR4W.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/01_21__walk_down_hall_angry__03X7CELV.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/02_01__secret_conversation__YVGY8LOK.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/02_06__podium_speech_happy__N8OSN8P6.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/02_07__walking_down_street_outside_angry__O4SXNLRL.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/02_15__secret_conversation__MZWH8ATN.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/02_15__talking_against_wall__HTG660F8.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/02_15__walking_and_outside_surprised__MZWH8ATN.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/02_21__talking_angry_couch__Z0XHPQAR.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/03_07__walking_down_indoor_hall_disgust__PWXXULHR.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/03_11__exit_phone_room__P08VGHTA.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/03_14__walk_down_hall_angry__P1L5PF4I.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/03_15__kitchen_pan__AIOM1U5V.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/03_18__walking_and_outside_surprised__8HNSXOBW.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/03_21__kitchen_pan__YCSEBZO4.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/04_01__kitchen_still__6I623VU9.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/04_06__kitchen_still__ZK95PQDE.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/04_07__exit_phone_room__ITC0C48B.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/04_13__podium_speech_happy__00T3UYOR.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/04_13__walking_down_street_outside_angry__00T3UYOR.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/04_21__walking_down_street_outside_angry__5Y31RZP8.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/05_16__exit_phone_room__B62WCGUN.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/06_03__talking_against_wall__4I8LRXWF.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/06_07__exit_phone_room__NMGYPBXE.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/06_18__outside_talking_still_laughing__M36D0OJT.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/06_18__podium_speech_happy__DEA1TCLN.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/06_25__outside_talking_pan_laughing__MI9BDQ7M.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/07_02__talking_angry_couch__1H07DFQJ.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/08_16__podium_speech_happy__8Q7JCS95.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/fake/09_25__walking_down_street_outside_angry__5041ODBN.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/01__exit_phone_room.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/01__hugging_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/01__outside_talking_pan_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/01__outside_talking_still_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/03__hugging_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/03__kitchen_pan.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/03__podium_speech_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/04__exit_phone_room.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/04__outside_talking_still_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/05__kitchen_pan.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/05__walk_down_hall_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/06__talking_angry_couch.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/06__walk_down_hall_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/06__walking_outside_cafe_disgusted.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/07__hugging_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/07__kitchen_pan.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/08__walking_down_street_outside_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/10__exit_phone_room.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/11__podium_speech_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/11__talking_against_wall.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/11__talking_angry_couch.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/12__talking_angry_couch.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/12__walking_and_outside_surprised.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/13__outside_talking_pan_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/14__outside_talking_pan_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/14__walking_down_indoor_hall_disgust.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/15__exit_phone_room.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/15__kitchen_pan.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/15__talking_against_wall.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Test/real/15__walk_down_hall_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/01_02__outside_talking_still_laughing__YVGY8LOK.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/01_02__walk_down_hall_angry__YVGY8LOK.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/01_03__hugging_happy__ISF9SP4G.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/01_03__podium_speech_happy__480LQD1C.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/01_03__talking_against_wall__JZUXXFRB.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/01_11__meeting_serious__9OM3VE0Y.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/01_11__secret_conversation__4OJNJLOO.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/01_11__talking_against_wall__9229VVZ3.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/01_12__outside_talking_pan_laughing__TNI7KUZ6.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/01_20__outside_talking_pan_laughing__OTGHOG4Z.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/01_20__outside_talking_still_laughing__FW94AIMJ.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/01_27__hugging_happy__ZYCZ30C0.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/01_27__outside_talking_still_laughing__ZYCZ30C0.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/01_27__walking_outside_cafe_disgusted__ZYCZ30C0.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_03__walking_down_street_outside_angry__QH3Y0IG0.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_03__walking_outside_cafe_disgusted__QH3Y0IG0.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_06__talking_angry_couch__GH8TGTBS.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_06__talking_angry_couch__MKZTXQ2T.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_06__walking_and_outside_surprised__N8OSN8P6.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_06__walking_down_indoor_hall_disgust__U6MDWIHG.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_07__meeting_serious__1JCLEEBQ.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_07__walk_down_hall_angry__U7DEOZNV.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_07__walking_and_outside_surprised__1VMZUH1W.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_09__exit_phone_room__HIH8YA82.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_09__kitchen_pan__HIH8YA82.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_12__podium_speech_happy__9D2ZHEKW.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_13__exit_phone_room__CP5HFV3K.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_13__outside_talking_pan_laughing__2YSYT2N3.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_13__podium_speech_happy__2YSYT2N3.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_13__secret_conversation__CP5HFV3K.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_14__outside_talking_pan_laughing__3IUBEKCT.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_15__exit_phone_room__I8G2LWD1.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_15__exit_phone_room__MZWH8ATN.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_15__outside_talking_pan_laughing__I8G2LWD1.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_15__outside_talking_pan_laughing__SB6PMCO0.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_15__talking_angry_couch__HTG660F8.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_15__talking_angry_couch__I8G2LWD1.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_15__walk_down_hall_angry__TN2CWM3K.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_15__walking_and_outside_surprised__HTG660F8.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_15__walking_and_outside_surprised__I8G2LWD1.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_18__exit_phone_room__OXMEEFUQ.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_18__outside_talking_pan_laughing__OXMEEFUQ.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_18__walking_down_street_outside_angry__21JTDDEL.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_21__kitchen_pan__Z0XHPQAR.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_25__talking_against_wall__Z7FQ69VP.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_27__hugging_happy__GVFLSZD5.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/02_27__walk_down_hall_angry__78M8S6M6.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_01__walking_and_outside_surprised__JZUXXFRB.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_04__outside_talking_pan_laughing__T04P6ELC.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_04__talking_angry_couch__T04P6ELC.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_06__meeting_serious__83ABVHC3.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_06__podium_speech_happy__83ABVHC3.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_07__hugging_happy__BKLOCI1M.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_07__walk_down_hall_angry__6PHZRQ4H.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_07__walk_down_hall_angry__IFSURI9X.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_07__walking_outside_cafe_disgusted__CDSNLDQ8.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_07__walking_outside_cafe_disgusted__IFSURI9X.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_07__walking_outside_cafe_disgusted__PWXXULHR.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_09__outside_talking_still_laughing__RCETIXYL.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_09__secret_conversation__RCETIXYL.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_11__talking_against_wall__P08VGHTA.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_13__kitchen_pan__GBYWJW06.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_13__meeting_serious__GBYWJW06.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_13__meeting_serious__T3MZOI8X.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_13__outside_talking_still_laughing__GBYWJW06.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_14__podium_speech_happy__Q9NSXM88.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_14__talking_against_wall__ZC2KYASW.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_14__walking_down_street_outside_angry__H0VQHGS3.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_15__kitchen_pan__DNUJD8M2.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_15__outside_talking_pan_laughing__Y11NT1YX.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_15__outside_talking_still_laughing__DNUJD8M2.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_15__podium_speech_happy__DG8ITQO3.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_18__walking_outside_cafe_disgusted__22UBC0BS.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_21__exit_phone_room__YCSEBZO4.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_21__meeting_serious__V53E3RVB.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_21__secret_conversation__YCSEBZO4.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_26__kitchen_pan__WBSXBL82.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_26__kitchen_still__WBSXBL82.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_26__talking_against_wall__WBSXBL82.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_27__walk_down_hall_angry__IL675GCI.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/03_27__walking_down_indoor_hall_disgust__IL675GCI.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/04_01__exit_phone_room__0XUW13RW.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/04_01__outside_talking_still_laughing__0XUW13RW.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/04_01__outside_talking_still_laughing__6I623VU9.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/04_01__secret_conversation__6I623VU9.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/04_07__kitchen_pan__XRK7FGZX.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/04_12__walking_down_street_outside_angry__96TQKDFJ.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/04_13__secret_conversation__00T3UYOR.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/04_13__walking_outside_cafe_disgusted__00T3UYOR.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/04_18__kitchen_still__NAXINA1N.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/04_26__outside_talking_still_laughing__WX836VLY.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/05_08__talking_against_wall__PRBCE28Z.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/05_16__walk_down_hall_angry__OSXCUOHX.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/05_16__walk_down_hall_angry__U9WZI5LK.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/05_17__hugging_happy__YTJYYDO9.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/05_17__walking_down_street_outside_angry__M3H96PDQ.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/05_28__exit_phone_room__U9LRLJ6N.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/05_28__kitchen_still__W3J028UG.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_02__podium_speech_happy__N8OSN8P6.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_02__walk_down_hall_angry__37DH75GQ.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_04__outside_talking_still_laughing__ZK95PQDE.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_04__walking_outside_cafe_disgusted__ZK95PQDE.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_07__kitchen_pan__NMGYPBXE.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_07__walking_down_street_outside_angry__NMGYPBXE.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_11__walking_outside_cafe_disgusted__MX659QU8.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_12__outside_talking_pan_laughing__3K21NFNM.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_12__podium_speech_happy__0VR4Y891.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_12__podium_speech_happy__3K21NFNM.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_14__outside_talking_pan_laughing__8U9ULZDT.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_14__walking_and_outside_surprised__8U9ULZDT.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_14__walking_down_indoor_hall_disgust__8U9ULZDT.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_15__exit_phone_room__QRCD27P8.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_15__outside_talking_still_laughing__QRCD27P8.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_15__walking_and_outside_surprised__QRCD27P8.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_18__outside_talking_pan_laughing__DEA1TCLN.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_18__walk_down_hall_angry__LH0KWJKM.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_20__kitchen_pan__6SUW7063.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_25__talking_angry_couch__MI9BDQ7M.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_25__walking_and_outside_surprised__MI9BDQ7M.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_26__kitchen_pan__L5BVR5L9.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_26__kitchen_still__L5BVR5L9.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_27__talking_angry_couch__JOG5PB18.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/06_27__walking_and_outside_surprised__O7L5Z9U8.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_02__walking_down_street_outside_angry__O4SXNLRL.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_03__hugging_happy__7NGMD8FT.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_03__outside_talking_pan_laughing__IFSURI9X.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_03__podium_speech_happy__6PHZRQ4H.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_03__secret_conversation__IFSURI9X.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_03__talking_angry_couch__WPT3Z2KN.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_03__walking_outside_cafe_disgusted__F0YYEA5W.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_06__kitchen_pan__NMGYPBXE.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_06__outside_talking_still_laughing__NMGYPBXE.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_06__walking_down_street_outside_angry__NMGYPBXE.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_09__outside_talking_still_laughing__N9CWME71.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_09__walk_down_hall_angry__N9CWME71.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_12__outside_talking_still_laughing__0VN0A2T3.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_13__walking_outside_cafe_disgusted__RVQCPCJF.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_14__exit_phone_room__P9QFO50U.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_14__talking_against_wall__P9QFO50U.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_15__hugging_happy__9Z2MLKVX.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_20__outside_talking_pan_laughing__KV6Q7D6C.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_20__talking_against_wall__KV6Q7D6C.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_21__exit_phone_room__K7KXUHMU.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_21__outside_talking_still_laughing__K7KXUHMU.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_21__talking_against_wall__MKU99DVX.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_25__walk_down_hall_angry__PAE9HCA8.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_26__walk_down_hall_angry__FGNGC2GT.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_26__walking_down_street_outside_angry__FGNGC2GT.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/07_27__walking_down_street_outside_angry__3RH5PR6S.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/08_05__walk_down_hall_angry__FBICSP2C.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/08_28__outside_talking_pan_laughing__8BC35RFU.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/09_01__talking_against_wall__O8HNNX43.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/09_01__walk_down_hall_angry__6TSGVLHA.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/09_02__walk_down_hall_angry__6KUOFMZW.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/09_02__walking_down_street_outside_angry__6KUOFMZW.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/09_02__walking_down_street_outside_angry__9TDCEK1Q.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/09_03__kitchen_pan__8DTEGQ54.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/09_03__talking_against_wall__8DTEGQ54.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/09_07__kitchen_pan__N9CWME71.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/09_13__kitchen_pan__21H6XSPE.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/09_13__outside_talking_pan_laughing__LPT427RY.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/09_18__outside_talking_pan_laughing__3VP8836C.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/09_20__kitchen_pan__98NUQ3E6.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/09_20__podium_speech_happy__O5X0AWR9.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/09_20__talking_angry_couch__0IRM5ADD.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/09_26__talking_against_wall__C3K20JOL.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/09_26__walk_down_hall_angry__QSE5A0GF.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/10_19__kitchen_still__IDX76N5R.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/10_22__kitchen_pan__EHARPYBT.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/fake/10_22__kitchen_still__EHARPYBT.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/01__kitchen_pan.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/01__kitchen_still.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/01__meeting_serious.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/01__podium_speech_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/01__secret_conversation.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/01__talking_against_wall.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/01__talking_angry_couch.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/01__walk_down_hall_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/01__walking_and_outside_surprised.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/01__walking_down_indoor_hall_disgust.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/01__walking_down_street_outside_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/01__walking_outside_cafe_disgusted.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/02__exit_phone_room.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/02__hugging_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/02__kitchen_pan.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/02__kitchen_still.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/02__meeting_serious.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/02__outside_talking_pan_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/02__outside_talking_still_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/02__podium_speech_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/02__secret_conversation.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/02__talking_against_wall.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/02__talking_angry_couch.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/02__walk_down_hall_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/02__walking_and_outside_surprised.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/02__walking_down_indoor_hall_disgust.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/02__walking_down_street_outside_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/02__walking_outside_cafe_disgusted.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/03__exit_phone_room.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/03__kitchen_still.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/03__meeting_serious.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/03__outside_talking_pan_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/03__outside_talking_still_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/03__secret_conversation.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/03__talking_against_wall.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/03__talking_angry_couch.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/03__walk_down_hall_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/03__walking_and_outside_surprised.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/03__walking_down_indoor_hall_disgust.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/03__walking_down_street_outside_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/03__walking_outside_cafe_disgusted.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/04__kitchen_pan.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/04__kitchen_still.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/04__outside_talking_pan_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/04__podium_speech_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/04__secret_conversation.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/04__talking_against_wall.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/04__talking_angry_couch.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/04__walk_down_hall_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/04__walking_down_street_outside_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/04__walking_outside_cafe_disgusted.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/05__exit_phone_room.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/05__hugging_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/05__kitchen_still.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/05__outside_talking_pan_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/05__outside_talking_still_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/05__podium_speech_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/05__talking_against_wall.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/05__walking_down_street_outside_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/05__walking_outside_cafe_disgusted.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/06__exit_phone_room.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/06__hugging_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/06__kitchen_pan.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/06__kitchen_still.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/06__outside_talking_pan_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/06__outside_talking_still_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/06__podium_speech_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/06__talking_against_wall.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/06__walking_and_outside_surprised.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/06__walking_down_indoor_hall_disgust.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/06__walking_down_street_outside_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/07__exit_phone_room.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/07__kitchen_still.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/07__outside_talking_pan_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/07__outside_talking_still_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/07__podium_speech_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/07__secret_conversation.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/07__talking_against_wall.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/07__talking_angry_couch.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/07__walk_down_hall_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/07__walking_down_street_outside_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/07__walking_outside_cafe_disgusted.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/08__exit_phone_room.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/08__kitchen_pan.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/08__kitchen_still.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/08__outside_talking_pan_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/08__outside_talking_still_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/08__podium_speech_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/08__talking_against_wall.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/08__walk_down_hall_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/08__walking_outside_cafe_disgusted.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/09__exit_phone_room.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/09__kitchen_pan.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/09__outside_talking_pan_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/09__outside_talking_still_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/09__podium_speech_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/09__talking_against_wall.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/09__talking_angry_couch.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/09__walk_down_hall_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/09__walking_down_street_outside_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/10__kitchen_pan.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/10__kitchen_still.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/10__outside_talking_pan_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/10__outside_talking_still_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/10__podium_speech_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/10__talking_against_wall.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/10__talking_angry_couch.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/10__walk_down_hall_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/10__walking_down_street_outside_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/10__walking_outside_cafe_disgusted.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/11__exit_phone_room.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/11__kitchen_pan.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/11__kitchen_still.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/11__outside_talking_pan_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/11__outside_talking_still_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/11__secret_conversation.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/11__walk_down_hall_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/11__walking_down_street_outside_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/11__walking_outside_cafe_disgusted.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/12__exit_phone_room.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/12__hugging_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/12__kitchen_pan.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/12__kitchen_still.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/12__outside_talking_pan_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/12__outside_talking_still_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/12__podium_speech_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/12__secret_conversation.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/12__talking_against_wall.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/12__walk_down_hall_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/12__walking_down_indoor_hall_disgust.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/12__walking_down_street_outside_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/12__walking_outside_cafe_disgusted.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/13__exit_phone_room.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/13__hugging_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/13__kitchen_pan.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/13__kitchen_still.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/13__outside_talking_still_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/13__podium_speech_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/13__secret_conversation.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/13__talking_against_wall.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/13__talking_angry_couch.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/13__walk_down_hall_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/13__walking_and_outside_surprised.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/13__walking_down_indoor_hall_disgust.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/13__walking_down_street_outside_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/13__walking_outside_cafe_disgusted.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/14__exit_phone_room.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/14__hugging_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/14__kitchen_pan.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/14__kitchen_still.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/14__outside_talking_still_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/14__podium_speech_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/14__secret_conversation.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/14__talking_against_wall.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/14__talking_angry_couch.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/14__walk_down_hall_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/14__walking_and_outside_surprised.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/14__walking_down_street_outside_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/14__walking_outside_cafe_disgusted.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/15__hugging_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/15__kitchen_still.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/15__outside_talking_pan_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/15__outside_talking_still_laughing.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/15__podium_speech_happy.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/15__talking_angry_couch.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/15__walking_and_outside_surprised.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/15__walking_down_indoor_hall_disgust.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/15__walking_down_street_outside_angry.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/15__walking_outside_cafe_disgusted.mp4  \n",
            "  inflating: ff-face-cropped/ModelData/Train/real/16__exit_phone_room.mp4  \n"
          ]
        }
      ],
      "source": [
        "!unzip ff-face-cropped.zip -d ff-face-cropped/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC0XugTfon2r"
      },
      "source": [
        "#CSV Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cl3ER4mIoK2z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "def get_video_data(folder_path, tag):\n",
        "    \n",
        "    video_data = []\n",
        "    for idx, video_name in enumerate(os.listdir(folder_path)):\n",
        "        video_path = os.path.join(folder_path, video_name)\n",
        "        if os.path.isfile(video_path):\n",
        "            video_data.append((idx, video_path, tag))\n",
        "    return video_data\n",
        "\n",
        "def create_csv_file(output_csv, data):\n",
        "    with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Index', 'Video Name', 'Tag'])\n",
        "        writer.writerows(data)\n",
        "\n",
        "def main():\n",
        "    train_folder = '/content/ff-face-cropped/ModelData/Test'\n",
        "    output_csv = './test.csv'\n",
        "\n",
        "    real_folder = os.path.join(train_folder, 'real')\n",
        "    fake_folder = os.path.join(train_folder, 'fake')\n",
        "\n",
        "    real_videos = get_video_data(real_folder, 'real')\n",
        "    fake_videos = get_video_data(fake_folder, 'fake')\n",
        "\n",
        "    all_videos = real_videos + fake_videos\n",
        "    create_csv_file(output_csv, all_videos)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3sLcgpFpBwT"
      },
      "source": [
        "#Preprocessing for EfficientNetB0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UF5EEIVCox4W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, GlobalAveragePooling2D, Input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "t1e_4abGpLT_"
      },
      "outputs": [],
      "source": [
        "#Load train and test csv files\n",
        "train_df = pd.read_csv('./train.csv')\n",
        "test_df = pd.read_csv('./test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9e7pbjKpSLV"
      },
      "source": [
        "## Extract Frames from video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQhq4TA1wdpp"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "\n",
        "def extract_frames(video_path, num_frames=30, target_size=(224, 224)):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_interval = max(1, total_frames // num_frames)\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_interval)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.convertScaleAbs(frame, alpha=1.2, beta=10)\n",
        "\n",
        "        frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        frame = preprocess_input(frame.astype(np.float32))\n",
        "\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "    return np.stack(frames, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vNZkzsK5pXA4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "import numpy as np\n",
        "\n",
        "class VideoDataGenerator(Sequence):\n",
        "    def __init__(self, df, batch_size=4, num_frames=30, target_size=(224, 224), shuffle=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.batch_size = batch_size\n",
        "        self.num_frames = num_frames\n",
        "        self.target_size = target_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indices = np.arange(len(self.df))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.df) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_df = self.df.iloc[batch_indices]\n",
        "\n",
        "        X, y = [], []\n",
        "        for _, row in batch_df.iterrows():\n",
        "            frames = extract_frames(row['Video Name'], self.num_frames, self.target_size)\n",
        "            if frames.shape[0] == self.num_frames:\n",
        "                X.append(frames)\n",
        "                y.append(1 if row['Tag'].lower() == 'fake' else 0)\n",
        "\n",
        "        return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRx2M6fFpaGA"
      },
      "outputs": [],
      "source": [
        "#Prepare datasets\n",
        "train_gen = VideoDataGenerator(train_df, batch_size=4)\n",
        "test_gen = VideoDataGenerator(test_df, batch_size=4, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQnTq9juphC5"
      },
      "source": [
        "## Defining Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THRNSWjwpekN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import Input, TimeDistributed, GlobalAveragePooling2D, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Input shape: 30 frames of size 224x224x3\n",
        "input_layer = Input(shape=(30, 224, 224, 3))\n",
        "\n",
        "# EfficientNetB0 base model for feature extraction\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "# Apply EfficientNetB0 to each frame\n",
        "x = TimeDistributed(base_model)(input_layer)\n",
        "x = TimeDistributed(GlobalAveragePooling2D())(x)\n",
        "\n",
        "# LSTM layers for temporal modeling\n",
        "x = LSTM(128, return_sequences=True)(x)\n",
        "x = LSTM(64, return_sequences=False)(x)\n",
        "\n",
        "# Fully connected layers\n",
        "x = Dense(64, activation='relu')(x)\n",
        "out = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Build and compile the model\n",
        "model = Model(inputs=input_layer, outputs=out)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQx9Mgydq3PK",
        "outputId": "f2395843-b2d9-4434-c7b5-3a6fc8e840cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 2s/step - accuracy: 0.5367 - loss: 0.6916 - val_accuracy: 0.6441 - val_loss: 0.6295\n",
            "Epoch 2/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 519ms/step - accuracy: 0.6813 - loss: 0.5958 - val_accuracy: 0.5932 - val_loss: 0.6573\n",
            "Epoch 3/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 539ms/step - accuracy: 0.7177 - loss: 0.5651 - val_accuracy: 0.6949 - val_loss: 0.5654\n",
            "Epoch 4/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 527ms/step - accuracy: 0.7472 - loss: 0.5190 - val_accuracy: 0.7458 - val_loss: 0.5923\n",
            "Epoch 5/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 530ms/step - accuracy: 0.7822 - loss: 0.5069 - val_accuracy: 0.7627 - val_loss: 0.5166\n",
            "Epoch 6/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 533ms/step - accuracy: 0.8021 - loss: 0.4286 - val_accuracy: 0.7627 - val_loss: 0.5241\n",
            "Epoch 7/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 518ms/step - accuracy: 0.8300 - loss: 0.4127 - val_accuracy: 0.6949 - val_loss: 0.5412\n",
            "Epoch 8/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 516ms/step - accuracy: 0.7672 - loss: 0.4843 - val_accuracy: 0.7288 - val_loss: 0.4858\n",
            "Epoch 9/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 525ms/step - accuracy: 0.8471 - loss: 0.3626 - val_accuracy: 0.7797 - val_loss: 0.5023\n",
            "Epoch 10/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 515ms/step - accuracy: 0.8486 - loss: 0.3482 - val_accuracy: 0.7458 - val_loss: 0.6562\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d97614a6b90>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_gen, validation_data=test_gen, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Guk4K6zysHVm"
      },
      "outputs": [],
      "source": [
        "model.save('EfficientNetB0_LSTM.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "csIkU8nswIHD",
        "outputId": "d27db8be-c992-4b38-967e-c1950c165f23"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">721,408</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_2 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │       \u001b[38;5;34m4,049,571\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_3 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1280\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m721,408\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m49,408\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,374,696</span> (24.32 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,374,696\u001b[0m (24.32 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">775,041</span> (2.96 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m775,041\u001b[0m (2.96 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,550,084</span> (5.91 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,550,084\u001b[0m (5.91 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgYeeCIH00ZY"
      },
      "source": [
        "# Updated model architecture with residual layer, bsae model being trainable and one dropout layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOZuwtLy0NDV"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import Input, TimeDistributed, GlobalAveragePooling2D, LSTM, Dense, Dropout, Add\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Input: 30 frames of 224x224 RGB images\n",
        "input_layer = Input(shape=(30, 224, 224, 3))\n",
        "\n",
        "# EfficientNetB0 base model\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "# Unfreeze last 5 layers\n",
        "for layer in base_model.layers[-5:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# TimeDistributed feature extraction\n",
        "x = TimeDistributed(base_model)(input_layer)\n",
        "x = TimeDistributed(GlobalAveragePooling2D())(x)\n",
        "\n",
        "# LSTM layers\n",
        "x = LSTM(128, return_sequences=True)(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = LSTM(64, return_sequences=False)(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# Dense + Residual\n",
        "res = Dense(64, activation='relu')(x)\n",
        "res = Dropout(0.3)(res)\n",
        "x = Add()([x, res])\n",
        "\n",
        "# Output\n",
        "out = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Model build\n",
        "model2 = Model(inputs=input_layer, outputs=out)\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "ebiCOVx31Hj7",
        "outputId": "5b3cc372-eb6b-4d71-81ca-c8deb2f8c4a6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ time_distributed_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)         │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ time_distributed_3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_distributed_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)         │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">721,408</span> │ time_distributed_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,   │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │ \u001b[38;5;34m3\u001b[0m)                     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ time_distributed_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m) │      \u001b[38;5;34m4,049,571\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)         │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ time_distributed_3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1280\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ time_distributed_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)         │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m721,408\u001b[0m │ time_distributed_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m49,408\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m4,160\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m65\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,824,612</span> (18.40 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,824,612\u001b[0m (18.40 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,556,481</span> (5.94 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,556,481\u001b[0m (5.94 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,268,131</span> (12.47 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,268,131\u001b[0m (12.47 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0-Y0ip61BQ5",
        "outputId": "a6e564e1-ae05-4e8c-d545-7fdb69068047"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 2s/step - accuracy: 0.5182 - loss: 0.7467 - val_accuracy: 0.5932 - val_loss: 0.6700\n",
            "Epoch 2/15\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 553ms/step - accuracy: 0.6420 - loss: 0.6461 - val_accuracy: 0.7119 - val_loss: 0.5805\n",
            "Epoch 3/15\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 560ms/step - accuracy: 0.7029 - loss: 0.5457 - val_accuracy: 0.7119 - val_loss: 0.5482\n",
            "Epoch 4/15\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 600ms/step - accuracy: 0.7817 - loss: 0.5241 - val_accuracy: 0.7797 - val_loss: 0.4719\n",
            "Epoch 5/15\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 567ms/step - accuracy: 0.7576 - loss: 0.4813 - val_accuracy: 0.7627 - val_loss: 0.4816\n",
            "Epoch 6/15\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 599ms/step - accuracy: 0.8207 - loss: 0.4379 - val_accuracy: 0.7797 - val_loss: 0.4498\n",
            "Epoch 7/15\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 550ms/step - accuracy: 0.8126 - loss: 0.3932 - val_accuracy: 0.7966 - val_loss: 0.3655\n",
            "Epoch 8/15\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 562ms/step - accuracy: 0.8799 - loss: 0.2820 - val_accuracy: 0.8136 - val_loss: 0.3697\n",
            "Epoch 9/15\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 560ms/step - accuracy: 0.8463 - loss: 0.3530 - val_accuracy: 0.7458 - val_loss: 0.5015\n",
            "Epoch 10/15\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 569ms/step - accuracy: 0.8834 - loss: 0.3452 - val_accuracy: 0.8644 - val_loss: 0.3456\n",
            "Epoch 11/15\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 562ms/step - accuracy: 0.8815 - loss: 0.2988 - val_accuracy: 0.8983 - val_loss: 0.3760\n",
            "Epoch 12/15\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 596ms/step - accuracy: 0.8776 - loss: 0.3271 - val_accuracy: 0.8475 - val_loss: 0.2853\n",
            "Epoch 13/15\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 558ms/step - accuracy: 0.8698 - loss: 0.2783 - val_accuracy: 0.7288 - val_loss: 0.5095\n",
            "Epoch 14/15\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 547ms/step - accuracy: 0.9056 - loss: 0.2403 - val_accuracy: 0.8983 - val_loss: 0.2113\n",
            "Epoch 15/15\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 554ms/step - accuracy: 0.9250 - loss: 0.2145 - val_accuracy: 0.8305 - val_loss: 0.2673\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e27a1994810>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2.fit(train_gen, validation_data=test_gen, epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D-eMAsC1J9Z"
      },
      "outputs": [],
      "source": [
        "model2.save('EfficientNetB0_LSTM_Modified.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5tyQ8o3Fxgi"
      },
      "source": [
        "## Lets try unfreezing more layers, more dropout, and data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbzFvXiZMo52"
      },
      "source": [
        "## Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf9ZK30hMosk"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "frame_augmenter = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "def augment_frame(frame):\n",
        "    if frame.dtype != np.float32:\n",
        "        frame = frame.astype(np.float32) / 255.0\n",
        "\n",
        "    frame = np.expand_dims(frame, axis=0)\n",
        "    augmented = frame_augmenter.flow(frame, batch_size=1, shuffle=False)[0][0]\n",
        "    return augmented\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebORtyndMohp"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "\n",
        "def extract_frames(video_path, num_frames=30, target_size=(224, 224), augment=False):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_interval = max(1, total_frames // num_frames)\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_interval)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.convertScaleAbs(frame, alpha=1.2, beta=10)\n",
        "\n",
        "        # Resize and interpolate\n",
        "        frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # Apply augmentation if enabled\n",
        "        if augment:\n",
        "            frame = augmenter(image=frame)\n",
        "\n",
        "        # Preprocess for EfficientNet\n",
        "        frame = preprocess_input(frame.astype(np.float32))\n",
        "\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "    return np.stack(frames, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6lxbB-oIMoZD"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "class VideoDataGenerator(Sequence):\n",
        "    def __init__(self, df, batch_size=4, num_frames=30, target_size=(224, 224), shuffle=True, augment=False):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.batch_size = batch_size\n",
        "        self.num_frames = num_frames\n",
        "        self.target_size = target_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.indices = np.arange(len(self.df))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.df) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_df = self.df.iloc[batch_indices]\n",
        "\n",
        "        X, y = [], []\n",
        "        for _, row in batch_df.iterrows():\n",
        "            frames = extract_frames(row['Video Name'], self.num_frames, self.target_size, augment=self.augment)\n",
        "            if frames.shape[0] == self.num_frames:\n",
        "                X.append(frames)\n",
        "                y.append(1 if row['Tag'].lower() == 'fake' else 0)\n",
        "\n",
        "        return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mhZFrUViNEC1"
      },
      "outputs": [],
      "source": [
        "#Prepare datasets\n",
        "train_gen = VideoDataGenerator(train_df, batch_size=4)\n",
        "test_gen = VideoDataGenerator(test_df, batch_size=4, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEF_53ahDKZv",
        "outputId": "280c2875-f05d-4aa5-e0af-8b8da462fb95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import Input, TimeDistributed, GlobalAveragePooling2D, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Input shape: 30 frames of size 224x224x3\n",
        "input_layer = Input(shape=(30, 224, 224, 3))\n",
        "\n",
        "# EfficientNetB0 base model for feature extraction\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "# Unfreeze last 20 layers\n",
        "for layer in base_model.layers[-20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Apply EfficientNetB0 to each frame\n",
        "x = TimeDistributed(base_model)(input_layer)\n",
        "x = TimeDistributed(GlobalAveragePooling2D())(x)\n",
        "\n",
        "# LSTM layers with Dropout\n",
        "x = LSTM(128, return_sequences=True)(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = LSTM(64, return_sequences=False)(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# Fully connected layers with Dropout\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "out = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Build and compile the model\n",
        "model = Model(inputs=input_layer, outputs=out)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMUFKjGNNJ3T",
        "outputId": "3048e64a-8e76-4d48-9bf2-22c25211e9b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 2s/step - accuracy: 0.5855 - loss: 0.6905 - val_accuracy: 0.6271 - val_loss: 0.6406 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 598ms/step - accuracy: 0.7197 - loss: 0.5574 - val_accuracy: 0.7288 - val_loss: 0.5150 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 595ms/step - accuracy: 0.7050 - loss: 0.5701 - val_accuracy: 0.8136 - val_loss: 0.4283 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 594ms/step - accuracy: 0.8313 - loss: 0.4277 - val_accuracy: 0.8136 - val_loss: 0.4365 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 594ms/step - accuracy: 0.7500 - loss: 0.4652 - val_accuracy: 0.6441 - val_loss: 0.9319 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 598ms/step - accuracy: 0.7585 - loss: 0.5243 - val_accuracy: 0.8136 - val_loss: 0.4964 - learning_rate: 5.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 598ms/step - accuracy: 0.8755 - loss: 0.3304 - val_accuracy: 0.8814 - val_loss: 0.2962 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 649ms/step - accuracy: 0.8761 - loss: 0.3065 - val_accuracy: 0.8644 - val_loss: 0.2732 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 593ms/step - accuracy: 0.9236 - loss: 0.2313 - val_accuracy: 0.8814 - val_loss: 0.2761 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 599ms/step - accuracy: 0.9326 - loss: 0.1818 - val_accuracy: 0.9153 - val_loss: 0.1865 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 596ms/step - accuracy: 0.9190 - loss: 0.1960 - val_accuracy: 0.8983 - val_loss: 0.2483 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 599ms/step - accuracy: 0.9300 - loss: 0.1794 - val_accuracy: 0.9322 - val_loss: 0.1900 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 608ms/step - accuracy: 0.9226 - loss: 0.2400 - val_accuracy: 0.9492 - val_loss: 0.1795 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 606ms/step - accuracy: 0.9817 - loss: 0.1038 - val_accuracy: 0.9322 - val_loss: 0.2196 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 607ms/step - accuracy: 0.9493 - loss: 0.1342 - val_accuracy: 0.9153 - val_loss: 0.2187 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 602ms/step - accuracy: 0.9488 - loss: 0.1535 - val_accuracy: 0.9492 - val_loss: 0.1962 - learning_rate: 1.2500e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 647ms/step - accuracy: 0.9773 - loss: 0.0740 - val_accuracy: 0.9322 - val_loss: 0.1825 - learning_rate: 1.2500e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 607ms/step - accuracy: 0.9813 - loss: 0.0851 - val_accuracy: 0.9322 - val_loss: 0.2090 - learning_rate: 6.2500e-05\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78fcdc4a5690>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_gen,\n",
        "          validation_data=test_gen,\n",
        "          epochs=50,\n",
        "          callbacks=[early_stop, lr_scheduler])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OgFsx5HANSBH"
      },
      "outputs": [],
      "source": [
        "model.save('EfficientNetB0_LSTM_Final.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_3r2IuaTIiS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 7061276,
          "sourceId": 11293177,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "012cb76f42234ea89eb229e288f75ede": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01487909a1444600a4487123a0493f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec38522499024cf89d5c45f6618dc64d",
            "placeholder": "​",
            "style": "IPY_MODEL_94c82cbc0bf74da8a0836482bc5d7b5b",
            "value": "Connecting..."
          }
        },
        "0e9659389d274aa7856a56deb9ecb827": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f3f8011403b4ad0ae0355fa7aed15c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f044bff5a34425f900909963ad46b85",
            "placeholder": "​",
            "style": "IPY_MODEL_6fc95e927b36495893010e89395d9f68",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "220cc57c613f4007ae5b65f6d82170a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc6a1140e63e4ebba4b348abcc25647d",
            "placeholder": "​",
            "style": "IPY_MODEL_32b50850a00c4b0298db2eab75ad53d4",
            "value": "Kaggle credentials successfully validated."
          }
        },
        "268a7b0787d2480688902d8998b7b7ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_012cb76f42234ea89eb229e288f75ede",
            "style": "IPY_MODEL_3daaa0b7e8ad44b3990d20223cdfb905",
            "tooltip": ""
          }
        },
        "2f044bff5a34425f900909963ad46b85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fa383561b6c40d59a8760298fa3bdf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_220cc57c613f4007ae5b65f6d82170a7"
            ],
            "layout": "IPY_MODEL_cf6a4b9026ee4bf1846befa6a7c99337"
          }
        },
        "32b50850a00c4b0298db2eab75ad53d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3daaa0b7e8ad44b3990d20223cdfb905": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "4a96413bc8fd4eed9743c72b462a11c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7b5ef3c68ebd4dbaabcb409cb23d3251",
            "placeholder": "​",
            "style": "IPY_MODEL_f759465bf8634f26b4ef4f617fdfe712",
            "value": ""
          }
        },
        "56fd10c50e0e4ea8b9fc9a4d2783d701": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_a14b698cd0bb4ae3b77695b42213c86e",
            "placeholder": "​",
            "style": "IPY_MODEL_ac651447ac9641fd8ca142f9bb8a234e",
            "value": "kavyasoni99"
          }
        },
        "6fc95e927b36495893010e89395d9f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b5ef3c68ebd4dbaabcb409cb23d3251": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94c82cbc0bf74da8a0836482bc5d7b5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a14b698cd0bb4ae3b77695b42213c86e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac651447ac9641fd8ca142f9bb8a234e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc6a1140e63e4ebba4b348abcc25647d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf6a4b9026ee4bf1846befa6a7c99337": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "cfe68a78a98b41e8aa0f1a7e65245c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1c33b5aef724f1793b61d64b03d37f0",
            "placeholder": "​",
            "style": "IPY_MODEL_0e9659389d274aa7856a56deb9ecb827",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "d1c33b5aef724f1793b61d64b03d37f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec38522499024cf89d5c45f6618dc64d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f759465bf8634f26b4ef4f617fdfe712": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
